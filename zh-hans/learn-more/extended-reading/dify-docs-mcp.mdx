---
title: 部署 Dify Docs MCP 服务，与文档进行对话
description: 学习如何部署本地文档 MCP 服务器，并通过 AI 与文档进行对话。
---

Model Context Protocol (MCP) 是由 Anthropic 开发的开放标准协议，为 AI 与外部工具和数据源提供标准化交互接口。AI 通过此协议访问软件工具和数据资源。

本文介绍如何基于 [dify-docs-mintlify 仓库](https://github.com/langgenius/dify-docs-mintlify) 部署本地 MCP 服务器，实现 LLM 与文档的自然对话。

<Info>
    详细了解 MCP 协议，请参考 [Model Context Protocol](https://docs.anthropic.com/en/docs/agents-and-tools/mcp)。
</Info>

<Accordion title="工作原理">
**1. 分析问题**

接收到问题后，AI 会先分析其中涉及的概念和技术细节，判断是否需要查阅 Dify 的官方文档。

**2. 生成查询关键词**

如果需要文档支持，AI 会根据问题的核心内容，自动生成搜索关键词和查询策略。比如询问“如何在 Dify 中集成 OpenAI GPT-4 模型”，AI 可能会生成“模型配置”、“API 集成”、“GPT-4 设置”等关键词，用于精准搜索。

**3. 检索文档内容**

MCP 服务器接收到这些查询后，会在 Dify 的文档库中进行搜索，并返回最相关的内容。

**4. 整合生成答案**

AI 将检索到的信息进行整理和分析，最终生成一个准确、详细的回答。

**5. 多轮搜索优化**

如果初次搜索结果不够完整，AI 会根据结果自动调整搜索策略，进行进一步查询，直到获取更完整的答案。
</Accordion>

## 前置准备

- Node ≥ 16.0
- Git
- MCP 客户端（如 Claude Desktop、VS Code 等）

## 安装步骤

### 1. 拉取仓库

克隆 Dify 文档仓库到本地：

```bash
git clone https://github.com/langgenius/dify-docs-mintlify.git
```

该仓库包含 Dify 完整文档，作为 MCP 服务器的数据源。

### 2. 初始化 MCP 服务器

进入文档目录，安装并配置 MCP 服务器：

```bash
cd dify-docs-mintlify
npx mint-mcp add dify-6c0370d8
```

此命令完成以下操作：
- 创建本地 MCP 服务器实例
- 分析和索引文档内容
- 配置必要依赖项

安装过程约需几分钟，时间取决于网络状况。

### 3. 配置 MCP 服务器

<Info>
    **MCP 客户端和服务器关系：**
    
    MCP 服务器提供知识和工具，处理查询请求、管理数据资源、执行特定功能。客户端为用户交互界面，通常是 AI 应用或聊天工具，接收用户问题，转发查询至相应 MCP 服务器，整合信息后返回用户。
</Info>

安装过程中选择 MCP 客户端。**以 Claude Desktop 为例**，工具生成相应配置信息。

![](https://assets-docs.dify.ai/2025/05/722c689b1c27d86c183c09dfda7a6a39.png)

完成安装后，在 Claude Desktop 设置中添加 MCP 服务器。进入设置页面，在 MCP 服务器配置部分添加服务器地址和端口，保存配置并重启应用。

![](https://assets-docs.dify.ai/2025/05/a332247103fb9a7dcf5b1a891979ce86.png)

## 与文档对话

配置完成后即可与 Dify 文档进行对话。该方式支持自然语言查询，无需逐页搜索或精确匹配关键词。

例如询问”我想创建一个客服聊天机器人，应该选择 Workflow 还是 Chatflow？“，系统解释两种应用类型区别，并结合客服场景给出具体建议。

![](https://assets-docs.dify.ai/2025/05/90eab3ce20dfeac2e98770adc75fb8b2.png)

对于技术问题如“如何修改 Dify 端口号”，AI 提供具体操作步骤：

![](https://assets-docs.dify.ai/2025/05/c836ed0ebab0b66c07d505049b72ea5f.png)

MCP 方式允许用自然语言描述问题，即使不确定具体技术术语，只需描述期望效果即可获取相关信息。

## 该方案与 RAG 方法对比

| 特性 | RAG (检索增强生成) | MCP (Model Context Protocol) |
|------|-------------------|----------------------------|
| 工作原理 | 预先将文档分割成文本块，使用向量嵌入建立索引。用户提问时，系统计算问题与文本块相似度，选择相关块作为上下文生成答案。 | AI 根据问题需求主动生成查询策略，支持多轮交互查询，根据初次结果调整后续搜索。 |
| 优势 | 处理速度快，适合大规模静态文档集合。 | - 可访问完整文档内容，支持跨章节复杂查询<br/>- 文档更新后无需再次进行繁琐处理，重新生成 MCP 服务即可提供查询服务<br/>- 提供连贯完整的答案 |
| 局限性 | - 静态文档分割可能分散相关信息<br/>- 基于向量相似度检索可能遗漏语义相关但词汇不同的内容<br/>- 上下文窗口限制，仅基于特定片段生成答案 | - 需要消耗大量 Tokens，成本较高<br/>- 依赖 LLM 的查询策略生成能力，可能影响检索准确性<br/>- 多轮交互查询可能导致响应时间较长<br/>- 需要额外的 MCP 服务器部署和维护成本 |
