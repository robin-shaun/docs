---
title: "LLM 配置与使用"
---

### **1. 为什么建议 max_tokens 设置小一点？**

因为在自然语言处理中，较长的文本输出通常需要更长的计算时间和更多的计算资源。因此，限制输出文本的长度可以在一定程度上降低计算成本和计算时间。例如设置：max_tokens=500 ，表示最多只考虑输出文本的前 500 个 token，而超过这个长度的部分将会被丢弃。这样做的目的是保证输出文本的长度不会超过 LLM 的接受范围，同时还可以充分利用计算资源，提高模型的运行效率。另一方面，更多的情况是，限制 max_tokens 能够增加 prompt 的长度，如 gpt-3.5-turbo 的限制为 4097 tokens，如果设置 max_tokens=4000，那么 prompt 就只剩下 97 tokens 可用，如果超过就会报错。

### **2 知识库长文本如何切分比较合理？**

在一些自然语言处理应用中，通常会将文本按照段落或者句子进行切分，以便更好地处理和理解文本中的语义和结构信息。最小切分单位取决于具体的任务和技术实现。例如：

- 对于文本分类任务，通常将文本按照句子或者段落进行切分
- 对于机器翻译任务，则需要将整个句子或者段落作为切分单位。

最后，还需要进行实验和评估来确定最合适的 embedding 技术和切分单位。可以在测试集上 / 命中测试比较不同技术和切分单位的性能表现，并选择最优的方案。

### 3. 我们在获取知识库分段时用的什么距离函数？

我们使用[余弦相似度](https://en.wikipedia.org/wiki/Cosine_similarity)。距离函数的选择通常无关紧要。OpenAI 嵌入被归一化为长度 1，这意味着：

仅使用点积可以稍微更快地计算余弦相似度

余弦相似度和欧几里德距离将导致相同的排名

- > 如果将归一化后的嵌入向量用于计算余弦相似度或欧几里德距离，并基于这些相似性度量对向量进行排序，得到的排序结果将是相同的。也就是说，无论是使用余弦相似度还是欧几里德距离来衡量向量之间的相似性，排序后的结果将是一致的。这是因为在归一化后，向量的长度不再影响它们之间的相对关系，只有方向信息被保留下来。因此，使用归一化的向量进行相似性度量时，不同的度量方法将得到相同的排序结果。在向量归一化后，将所有向量的长度缩放到 1，这意味着它们都处于单位长度上。单位向量只描述了方向而没有大小，因为它们的长度恒为 1。_具体原理可问 ChatGPT._

当嵌入向量被归一化为长度 1 后，计算两个向量之间的余弦相似度可以简化为它们的点积。因为归一化后的向量长度都为 1，点积的结果就等同于余弦相似度的结果。由于点积运算相对于其他相似度度量（如欧几里德距离）的计算速度更快，因此使用归一化的向量进行点积计算可以稍微提高计算效率。

### 4. 应用里切换模型使用时遇到如下报错，该怎么解决？

```JSON
Anthropic: Error code: 400 - f'error': f'type': "invalid request error, 'message': 'temperature: range: -1 or 0..1)
```

由于每个模型的参数取值不同，需要按照当前模型的该参数值范围设置。

### 5. 遇到如下报错提示，该如何解决？

```JSON
Query or prefix prompt is too long, you can reduce the preix prompt, or shrink the max token, or switch to a llm with a larger token limit size
```

在编排页参数设置里，调小“最大 token”的值即可。

### 6 用户在使用应用时遇到报错“Invalid token”，该怎么解决？

如果遇到报错为 “Invalid token”，你可尝试如下两种解决办法：

- 浏览器清除缓存（Cookies、Session Storage 和 Local Storage），重新访问；
- 二是重新生成一个 App 网址，重新网址进入即可。

### 7. 有什么方式能控制更多地使用上下文数据而不是模型自身生成能力吗？

是否使用知识库，会和知识库的描述有关系，尽可能把知识库描述写清楚。

### 8. 上传知识库文档是 Excel，该如何更好地分段？

首行设置表头，后面每行显示内容，不要有其他多余的表头设置，不要设置复杂格式的表格内容。

如下方表格示例，仅需保留第二行的表头，首行（表格1）为多余表头，需删掉。

![](https://assets-docs.dify.ai/dify-enterprise-mintlify/zh_CN/learn-more/faq/2bdf503188184f3c2973318e132160da.png)

{/*
Contributing Section
DO NOT edit this section!
It will be automatically generated by the script.
*/}

---